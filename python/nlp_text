##### NLP + String Formatting ######

# Read .txt as a string object
with open(file_name) as f:
    file_name_data = f.read()
file_name_data=file_name_data.lower()
# Find all the words and get into a list - think this handles the /n newline elements
words = re.findall('\w+',file_name_data)


# !pip install nltk 
import nltk
# nltk.download('punkt')
# nltk.download('averaged_perceptron_tagger')
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag

def preprocess(sent):
    sent = nltk.word_tokenize(sent)
    sent = nltk.pos_tag(sent) # Issue is that they all come out as proper nouns i.e. NNP 
    return sent


# spacy has a bunch of pipelines you can just install and load in - these can then do a bunch of preprocessing for you e.g. https://spacy.io/models/en
### THIS PIECE OF CODE IMPORTS A PREPROCESSING PIPELINE FROM SPACY THAT IS ABLE TO DO POS TAGGING AMONGST OTHER THINGS. 
### I USE IT TO ITERATE THROUGH VOICE SEARCHES. FOR EACH WORD IN THE VOICE SEARCH, I TAG IT WITH A POS ENTITY SO I CAN THEN IDENTIFY WHICH SEARCHES INCLUDE A PERSON NAME
import spacy
import en_core_web_sm  # English pipeline optimized for CPU. Components: tok2vec, tagger, parser, senter, ner, attribute_ruler, lemmatizer.
nlp = spacy.load("en_core_web_sm")

results = pd.DataFrame({'action_query':[],'Type':[]})
for search in searches:
    doc = nlp(search)
    for entity in doc.ents:
        results = results.append(dict(action_query = entity.text, Type = entity.label_), ignore_index=True)
results.loc[results['Type']=='PERSON', 'action_query'].tolist()

### USING CORPUSES OF NAMES FROM ONLINE TO IDENTIFY IF VOICE SEARCH RELATED TO AN ACTOR OR DIRECTOR  
female_names = pd.read_csv('https://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/female.txt')
male_names = pd.read_csv('https://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/male.txt')


# Alternatives to Levenshtein Distance to compare substrings that are essentially referring to the same thing # https://www.datacamp.com/community/tutorials/fuzzy-string-python
# Useful in spelling checks / mapping databases that lack a common key etc. 
from fuzzywuzzy import fuzz

# example 1 # 
Str1 = "Los Angeles Lakers"
Str2 = "Lakers"
Ratio = fuzz.ratio(Str1.lower(),Str2.lower())
Partial_Ratio = fuzz.partial_ratio(Str1.lower(),Str2.lower())
print(Ratio) # 50 
print(Partial_Ratio) # 100

# example 2 #
Str1 = "united states v. nixon"
Str2 = "Nixon v. United States"
Ratio = fuzz.ratio(Str1.lower(),Str2.lower())
Partial_Ratio = fuzz.partial_ratio(Str1.lower(),Str2.lower())
Token_Sort_Ratio = fuzz.token_sort_ratio(Str1,Str2)
print(Ratio) # 59
print(Partial_Ratio) # 74
print(Token_Sort_Ratio) # 100



##### 
import string
string.punctuation # lists all puncutation 

adj_suffix = ["able", "ese", "ful", "i", "ian", "ible", "ic", "ish", "ive", "less", "ly", "ous"]
any(word.endswith(suffix) for suffix in adj_suffix)

# To remove special characters: #
corpus = "learning% makes 'me' happy. i am happy be-cause i am learning! :)"
corpus = re.sub(r"[^a-zA-Z0-9.?! ]+", "", corpus)



calibrator = CalibratedClassifierCV(base_estimator=lgbm_clf, # stacking ensemble here??  
                                    cv='prefit',
                                   method='isotonic') # 'sigmoid'
calibrator.fit(X_train_oot, y_train_oot)
# evaluate the model
yhat = calibrator.predict_proba(X_test_oot)[:, 1]
