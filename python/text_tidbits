import re # for text manipualtion + regex 
import nltk
from nltk import word_tokenize
# nltk.download('punkt') # nltk.download('all')
nltk.download('stopwords')
from nltk.corpus import stopwords
## for bag-of-words
from sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing

def utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):
    '''
    Preprocess a string.
    :parameter
        :param text: string - name of column containing text
        :param lst_stopwords: list - list of stopwords to remove
        :param flg_stemm: bool - whether stemming is to be applied
        :param flg_lemm: bool - whether lemmitisation is to be applied
    :return
        cleaned text
    '''
    ## clean (convert to lowercase and remove punctuations and   
    characters and then strip)
    text = re.sub(r'[^\w\s]', '', str(text).lower().strip())
            
    ## Tokenize (convert from string to list)
    lst_text = text.split()
    ## remove Stopwords
    if lst_stopwords is not None:
        lst_text = [word for word in lst_text if word not in 
                    lst_stopwords]
                
    ## Stemming (remove -ing, -ly, ...)
    if flg_stemm == True:
        ps = nltk.stem.porter.PorterStemmer()
        lst_text = [ps.stem(word) for word in lst_text]
                
    ## Lemmatisation (convert the word into root word)
    if flg_lemm == True:
        lem = nltk.stem.wordnet.WordNetLemmatizer()
        lst_text = [lem.lemmatize(word) for word in lst_text]
            
    ## back to string from list
    text = " ".join(lst_text)
    return text

df['clean_text'] = df["raw_text"].apply(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True, lst_stopwords=lst_stopwords))

list_stopwords = nltk.corpus.stopwords.words("english")

################# BoW example ######################
## Count (classic BoW)
vectorizer = feature_extraction.text.CountVectorizer(max_features=10000, ngram_range=(1,2))
## Tf-Idf (advanced variant of BoW)
vectorizer = feature_extraction.text.TfidfVectorizer(max_features=10000, ngram_range=(1,2))

corpus = df['clean_text']
vectorizer.fit(corpus)
X_train = vectorizer.transform(corpus)
dic_vocabulary = vectorizer.vocabulary_
# to visualise the sparseness of the matrix # 
sns.heatmap(X_train.todense()[:,np.random.randint(0,X.shape[1],100)]==0, vmin=0, vmax=1, cbar=False).set_title('Sparse Matrix Sample')

# Could use chi-squared test to select features to reduce dimensionality - if multinomial, treat each category as binary and repear for n classes
# Chi-square determines if feature & target are independent 
y = df_train["y"]
X_names = vectorizer.get_feature_names()
p_value_limit = 0.95
dtf_features = pd.DataFrame()
for cat in np.unique(y):
    chi2, p = feature_selection.chi2(X_train, y==cat)
    dtf_features = dtf_features.append(pd.DataFrame(
                   {"feature":X_names, "score":1-p, "y":cat}))
    dtf_features = dtf_features.sort_values(["y","score"], 
                    ascending=[True,False])
    dtf_features = dtf_features[dtf_features["score"]>p_value_limit]
X_names = dtf_features["feature"].unique().tolist()

# Could then refit the vectorizer on the corpus containing the new set of words as inputs - to produce a smaller feature matrix & shorter vocab

# Could then train ML model - recommends Naive Bayes 
# NB suitable for large dataset as considers features independently, calculates probability of each class, and predicts argmax
classifier = naive_bayes.MultinomialNB()
model = pipeline.Pipeline([("vectorizer", vectorizer),  
                           ("classifier", classifier)])
model["classifier"].fit(X_train, y_train)
X_test = df_test["text_clean"].values
y_pred = model.predict(X_test)
y_predicted_prob = model.predict_proba(X_test)
# Now plot ROC /Confusion Matrix / PR-Curve 
## To understand why a single observation is predicted to be a certain class:
txt_instance = df_test["text"].iloc[1200] # chose observation 1200 as incorrectly classified - want to understand why 
print("True:", y_test[1200], "--> Pred:", y_pred[1200], "| Prob:", round(np.max(y_predicted_prob[1200]),2)) 
## show explanation
from lime import lime_text
explainer = lime_text.LimeTextExplainer(class_names=np.unique(y_train))
explained = explainer.explain_instance(txt_instance, model.predict_proba, num_features=3)
explained.show_in_notebook(text=txt_instance, predict_proba=False)


sentence.split()

words = word_tokenize("I love coding in Python")
print(words)
nltk.pos_tag(words) # Part of Speech Taggin - extracts grammatical structure e.g verb/noun/proposition

token_list = nltk.pos_tag(tokens) # 'JJ' adjectives
adj_list = [i for i, v in token_list if v == 'JJ']   # 'PRP','VBP', 'VBG', 'NNP'
# bag-of-words
watchmen_dictionary = {x:adj_list.count(x) for x in adj_list} 


# Display English stopwords
stop_words = stopwords.words('English')
print(stop_words)
# Display German stopwords
stop_words_GE = stopwords.words('German')
print(stop_words_GE)

sentence = "I am learning Python. It is one of the most popular programming languages"

sentence_words = word_tokenize(sentence)

sentence_no_stops = ' '.join([word for word in sentence_words if word not in stop_words])

# Text Normalisation : Different variations of text get converted into a standard form. For example, the words "does" and "doing" are converted to "do".

normalized_sentence = sentence.replace("US", "United States").replace("UK", "United Kingdom").replace("-18", "-2018")

# Stemming 
stemmer = nltk.stem.PorterStemmer()
stemmer.stem("production")
stemmer.stem("firing")

# Lemmatization # 
nltk.download('wordnet')
from nltk.stem.wordnet import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
lemmatizer.lemmatize('products')

# Named Entity Recognition (i.e. groups into people/places etc.)
nltk.download('maxent_ne_chunker')
i = nltk.ne_chunk(nltk.pos_tag(word_tokenize(sentence)), binary=True)
[a for a in i if len(a)==1]

# Homononyms - word sense disambiguation 
from nltk.wsd import lesk
sentence1 = "Keep your savings in the bank"
sentence2 = "It's so risky to drive over the banks of the road"
print(lesk(word_tokenize(sentence1), 'bank'))
print(lesk(word_tokenize(sentence2), 'bank'))



####
import spacy
spacy.load('en')
from spacy.lang.en import English
parser = English()
def tokenize(text):
    lda_tokens = []
    tokens = parser(text)
    for token in tokens:
        if token.orth_.isspace():
            continue
        elif token.like_url:
            lda_tokens.append('URL')
        elif token.orth_.startswith('@'):
            lda_tokens.append('SCREEN_NAME')
        else:
            lda_tokens.append(token.lower_)
    return lda_tokens



    import nltk
nltk.download('wordnet')
from nltk.corpus import wordnet as wn
def get_lemma(word):
    lemma = wn.morphy(word)
    if lemma is None:
        return word
    else:
        return lemma
    
from nltk.stem.wordnet import WordNetLemmatizer
def get_lemma2(word):
    return WordNetLemmatizer().lemmatize(word)



def prepare_text_for_lda(text):
    tokens = tokenize(text)
    tokens = [token for token in tokens if len(token) > 4]
    tokens = [token for token in tokens if token not in en_stop]
    tokens = [get_lemma(token) for token in tokens]
    return tokens


####
from gensim import corpora
dictionary = corpora.Dictionary(text_data)
corpus = [dictionary.doc2bow(text) for text in text_data]
import pickle
pickle.dump(corpus, open('corpus.pkl', 'wb'))
dictionary.save('dictionary.gensim')



NUM_TOPICS = 8
ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=50)
ldamodel.save('model5.gensim')
topics = ldamodel.print_topics(num_words=6)
for topic in topics:
    print(topic)

dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')
lda = gensim.models.ldamodel.LdaModel.load('model5.gensim')
import pyLDAvis.gensim
lda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)
pyLDAvis.display(lda_display)


### Sentiment Analysis ####
nltk.download('vader_lexicon')
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk import tokenize


sentences = ["VADER is smart.", # positive sentence (simple)
             "VADER is smart, handsome, and funny.", # positive sentence 
             "The book was kind of good.", # positive sentence (intensity adjusted)
             "A really bad, horrible book.",       # negative sentence with booster words
             "The plot was good, but the characters are uncompelling and the dialog is not great.", # mixed negation sentence
             "At least it isn't a horrible book.", # negated negative sentence with contraction
             ":) and :(",     # emoticons handled
             "",              # an empty string
             "Today sux",     #  negative slang
             "Today kinda sux! But I'll get by, lol" # mixed sentiment example with slang and constrastive conjunction "but"
            ]

sid = SentimentIntensityAnalyzer()

for sentence in sentences:
    print(sentence)
    ss = sid.polarity_scores(sentence)
    for k in sorted(ss):
        print('{0}: {1}, '.format(k, ss[k]), end='')
    print()

paragraph = "It was one of the worst movies I've seen, despite good reviews. \
Unbelievably bad acting!! Poor direction. VERY poor production. \
The movie was bad. VERY BAD!"

lines_list = tokenize.sent_tokenize(paragraph)

for sentence in lines_list:
    print(sentence)
    ss = sid.polarity_scores(sentence)
    for k in sorted(ss):
        print('{0}: {1}, '.format(k, ss[k]), end='')
    print()



    # Import TfidfVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

# tf   = frequency of the word in the document 
# idf  = weighting scheme that reduces the influence of frequent words like 'the'

# Create a TfidfVectorizer: tfidf
tfidf = TfidfVectorizer() 

# Apply fit_transform to document: csr_mat
csr_mat = tfidf.fit_transform(documents)

# Print result of toarray() method
print(csr_mat.toarray())

# Get the words: words
words = tfidf.get_feature_names()

# Print words
print(words)

##### To create the bag of Words #####
all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())    # bag-of-words created 
word_features = list(all_words)[:2000]                                 # only the 2000 most popular words will be used as features in the classifier 

##### How to extract names ???
import nltk
def extract_entities(text):
    for sent in nltk.sent_tokenize(text):
        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):
            if hasattr(chunk, 'node'):
                print chunk.node, ' '.join(c[0] for c in chunk.leaves())
