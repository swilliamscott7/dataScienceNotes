{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Notes\n","\n","- jar file = Java ARchive. file format based on ZIP used for aggregating many files into one\n","- Biggest issue was setting up BQ connector - needed a compatible .jar file according to scala version - https://github.com/GoogleCloudDataproc/spark-bigquery-connector\n","\n","- To submit a job directly to the cluster\n","    - $ gcloud dataproc jobs submit spark --jar gs://spark-lib/bigquery/spark-2.4-bigquery-0.28.0-preview.jar my_script.py --cluster clustertest2 --region europe-west1 \n","    - $ gcloud dataproc jobs submit spark --help\n","    \n","- SparkSession.builder.appName() did not work after a while and had to totally restart the cluster for some reason to get working again"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:root:'ARROW_PRE_0_15_IPC_FORMAT' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=0.15 and pyspark<3.0. Koalas will set it for you but it does not work if there is a Spark context already launched.\n"]}],"source":["# !pip install koalas # need to install if spark version < 3.2\n","\n","from pyspark.sql import SparkSession\n","import databricks.koalas as ks\n","import pandas as pd"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Scala code runner version 2.12.10 -- Copyright 2002-2019, LAMP/EPFL and Lightbend, Inc.\n","Welcome to\n","      ____              __\n","     / __/__  ___ _____/ /__\n","    _\\ \\/ _ \\/ _ `/ __/  '_/\n","   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.4.8\n","      /_/\n","                        \n","Using Scala version 2.12.10, OpenJDK 64-Bit Server VM, 1.8.0_352\n","Branch \n","Compiled by user  on 2023-01-03T22:09:32Z\n","Revision \n","Url \n","Type --help for more information.\n"]}],"source":["# 1.5.80-debian10\n","# https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-release-1.5 -- shows that it has spark version 2.4.8 \n","!scala -version\n","# think we want spark version of >3.2 - should be available with 2.0-centos8\n","!spark-sql --version # to get spark version"]},{"cell_type":"markdown","metadata":{},"source":["__To create dataproc cluster__"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# conf = SparkConf()\n","# conf.setMaster('spark://spark-master-svc.default.svc.cluster.local:7077')\n","# conf.setAppName('spark-basic')\n","# conf.set(\"spark.driver.host\", \"jupyter-headless.default.svc.cluster.local\")\n","# sc = SparkContext(conf=conf)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# suddenly taking a lot longer... not sure why # \n","spark = SparkSession.builder.appName('spark-bigquery-demo').config('spark.jars','gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar').getOrCreate()\n","# will output the results of DataFrames in each step without the new need to show df.show() and also improves the formatting of the output #\n","spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True)\n"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["# table = \"bigquery-public-data.wikipedia.pageviews_2020\"\n","\n","spark_df = spark.read.format(\"bigquery\").option(\"table\", 'sky-uk-ids-analytics-prod.uk_poc_beam_ic.BBDB_140k').load()\n"," \n","#     .option('header', 'true') \\\n","#     .option('delimiter', ',') \\\n","#     .option('inferSchema', 'true')\n","# spark_df.printSchema()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # Setup hadoop fs configuration for schema gs://\n","# conf = spark.sparkContext._jsc.hadoopConfiguration()\n","# conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n","# conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n","\n","# df.createOrReplaceTempView(\"words\")\n","\n","# csv_file_path = 'gs://YOUR_BUCKET/test.json'\n","# df = spark.read.json(json_file_path, schema, multiLine=True)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------+\n","|count(1)|\n","+--------+\n","|   67286|\n","+--------+\n","\n"]}],"source":["# If want to query this table directly from here : # \n","spark_df.createOrReplaceTempView(\"tempview\")\n","count = spark.sql(\"SELECT count(1) FROM tempview WHERE date_ = '2022-10-28' \")\n","count.show()\n","\n","### Saving the data to BigQuery\n","# count.write.format('bigquery').option('table', 'sky-uk-ids-analytics-prod.uk_poc_beam_ic.count').save()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","train_df = spark_df = spark.read.format(\"bigquery\").option(\"table\", 'project.uk_poc_beam_ic.Baseline_model_joint_UNSAMPLED') \\\n","  .load()\n","self.val_df = (self.client.query(dev_query.format(columns = self.parameters_file.COLUMNS,\n","                                  table = self.dev_dataset_name,\n","                                  identity_column = self.parameters_file.IDENTITY_COLUMN,\n","                                  splitter = self.parameters_file.VALIDATION_SPLIT))\n","                                  .result()\n","                                  .to_dataframe())\n","\n","self.test_df = (self.client.query(dev_query.format(columns = self.parameters_file.COLUMNS,\n","                                  table = self.dev_dataset_name,\n","                                  identity_column = self.parameters_file.IDENTITY_COLUMN,\n","                                  splitter = self.parameters_file.TEST_SPLIT))\n","                                  .result()\n","                                  .to_dataframe())\n","\n","self.oos_df = (self.client.query(oos_query.format(columns = self.parameters_file.COLUMNS,\n","                                  table = self.oos_dataset_name,\n","                                  identity_column = self.parameters_file.IDENTITY_COLUMN,\n","                                  splitter = self.parameters_file.OOS_SPLIT))\n","                                  .result()\n","                                  .to_dataframe())"]},{"cell_type":"markdown","metadata":{},"source":["__Convert to a koalas dataframe__"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'c:\\Users\\SSC24\\AppData\\Local\\Microsoft\\WindowsApps\\python3.7.exe' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'c:/Users/SSC24/AppData/Local/Microsoft/WindowsApps/python3.7.exe -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["ks.set_option('compute.default_index_type', 'distributed') # Need to set this OR 'distributed-sequence' to speed up .head() \n","\n","# Create a Koalas DataFrame from pandas DataFrame\n","# df_ = ks.from_pandas(df)\n","\n","kdf = spark_df.to_koalas(index_col=['account_number','date_']) # index_col might speed it up \n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"}},"nbformat":4,"nbformat_minor":4}
