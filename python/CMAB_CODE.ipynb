{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Adyen Contextual Multi-Armed Bandit (CMAB) Challenge\n",
    "\n",
    "- CMAB as better alternative to A/B testing\n",
    "- CMAB best described as a repeated interaction over a number of rounds $T$. Formally, at each round $t=1,2, \\ldots, T$:\n",
    "* The environment (i.e. the real world) reveals a context $x_t$ (i.e. payment features). \n",
    "* The learner chooses an action $a_t$ (i.e. an optimization). \n",
    "* The environment reveals a reward $r_t \\in \\{0,1\\}$ (i.e. an non-successful/successful payment). \n",
    "\n",
    "- Goal is to choose actions which maximize cumulative reward, that is $\\sum_{t=1}^T r_t$.\n",
    "- Specific Aim\n",
    "    - Build action selection algorithm that can pick best actions on the basis of past data \n",
    "    - Select actions for unseen data\n",
    "    - Output = `unseen_data.csv` with columns attached `y_pred`, `p(a)` and `a` \n",
    "- Data descriptions\n",
    "    - reference column (index: `ref`)\n",
    "    - reward label (`y`, a payment getting authorized)\n",
    "    - chosen action (`a`, an optimization decision)\n",
    "    - probability of an action being chosen (`p(a)`, the latent probability of an optimization being chosen)\n",
    "    - contextual features (`x`, payment related features)\n",
    "\n",
    "- Ordinarily would have a feedback loop to test algorithm, however in the context of this challenge, there is no explicit feedback loopB\n",
    "\n",
    "\n",
    "# Summary Conclusions\n",
    "\n",
    "- New action selection strategy when applied to OOT Test set performs better than current approach\n",
    "- Uplift of 17% with predicted Success rate of 74% using new strategy vs 63% using current approach\n",
    "- OOT Test Set refers to 10% of historic data that occurred on day  8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Install Packages__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lightgbm\n",
    "# !pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Load packages__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Optional, Dict\n",
    "\n",
    "import pandas as pd\n",
    "import lightgbm as lgbm\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score\n",
    "import numpy as np\n",
    "import pickle\n",
    "import _pickle as cPickle\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today_date = datetime.today().strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load user-defined functions\n",
    "\n",
    "- Used for training the oracle and finding the action selection policy based on the historic train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(probas_array):\n",
    "    \"\"\"\n",
    "    Computes softmax values for each sets of scores\n",
    "    Scaled probabilities sum to 1, to allow us to interpret each action probabilistically\n",
    "    If one value is far higher than the other it will dominate, squeezing the other probabilities within a small range\n",
    "    \"\"\"\n",
    "    exp_x = np.exp(probas_array - np.max(probas_array))\n",
    "    return exp_x / exp_x.sum(axis=0)\n",
    "\n",
    "def softmax_temp(probas_array, temp):\n",
    "    \"\"\"\n",
    "    Computes softmax values for each sets of scores\n",
    "    Scaled probabilities sum to 1, to allow us to interpret each action probabilistically\n",
    "    If one value is far higher than the other it will dominate, squeezing the other probabilities within a small range\n",
    "    \"\"\"\n",
    "    probas = [0]*len(probas_array)\n",
    "    for i in range(0,len(probas)):\n",
    "        probas[i] = np.exp(probas_array[i]/temp)\n",
    "    mapped_probas = np.array(probas/np.sum(probas))\n",
    "    return mapped_probas\n",
    "\n",
    "def action_selection_rec(df,\n",
    "                         method:str,\n",
    "                         epsilon=0.1,\n",
    "                         temp=0.02,\n",
    "                         seed=42,\n",
    "                         output=False,\n",
    "                         file=None) -> pd.DataFrame:\n",
    "    \"\"\" \n",
    "    df: dataframe containing refs, actions and probability of success for each respective action\n",
    "    method: eps_uniform, eps_weighted, softmax_temp\n",
    "        eps_uniform = Epsilon-greedy with uniformly sampled exploratory action\n",
    "        eps_weighted = Epsilon-greedy with weight-based sampling of exploratory action\n",
    "        softmax_temp = After applying the softmax transformation (including the addition of a \n",
    "                        temperature parameter that trades off between exploit vs explore), will sample actions using weights\n",
    "    return: dataframe with the recommended action for each ref, associated probability of success, and latent action probability\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    if method in ['eps_uniform','eps_weighted']:\n",
    "        \n",
    "        winning_indexes = df.groupby('ref')['y_pred'].idxmax().values\n",
    "        df.loc[winning_indexes,'max_action'] = 1\n",
    "        # To get scaled probas after excluding the winning strategy #\n",
    "        df.loc[df['max_action'] != 1,'p(a)_exploratory'] = df.loc[df['max_action'] != 1,].groupby('ref')['y_pred'].apply(lambda x: softmax(x))\n",
    "        exploit_refs = np.random.randint(df['ref'].min(), df['ref'].max(), int(len(df['ref'].unique())*(1-epsilon)) )\n",
    "        explore_refs = [i for i in df['ref'].unique() if i not in exploit_refs]\n",
    "        # For 1-epsilon of refs, choose action according to greedy approach\n",
    "        exploit_df = df.loc[(df['ref'].isin(exploit_refs)) & (df['max_action']==1),]\n",
    "\n",
    "        if method == 'eps_uniform':\n",
    "            explore_df = df.loc[(df['ref'].isin(explore_refs)) & (df['max_action']!=1), ].groupby('ref').apply(lambda x: x.sample(n=1))\n",
    "            \n",
    "        else:\n",
    "            # eps_weighted # \n",
    "            exploratory_idx = df.loc[(df['ref'].isin(explore_refs)) & (df['max_action'] != 1), ].groupby('ref').apply(lambda x: np.random.choice(x.index,1,p=x['p(a)_exploratory'].values))\n",
    "            explore_df = df.copy().loc[df.index.isin(exploratory_idx),]\n",
    "            \n",
    "        df_rec = pd.concat([explore_df,exploit_df],axis=0)\n",
    "        df_rec['a'] = df_rec[[i for i in df_rec.columns if 'choice' in i]].idxmax(axis=1)\n",
    "    \n",
    "    elif method == 'softmax_temp':\n",
    "    \n",
    "        df['p(a)'] = df.groupby('ref')['y_pred'].apply(lambda x: softmax_temp(x.values,temp)).explode().values\n",
    "        choice_indexes = df.groupby('ref').apply(lambda x: np.random.choice(x.index,1,p=x['p(a)']))\n",
    "        df_rec = df.copy().loc[df.index.isin(choice_indexes),]\n",
    "        df_rec['a'] = df_rec[[i for i in df_rec.columns if 'choice' in i]].idxmax(axis=1)\n",
    "        # What proportion of the times is the winning strategy chosen ? #\n",
    "        winning_strategy_prop = (df_rec['max_action'] == 1).sum() / len(df_rec)\n",
    "        print('Proportion of times winning strategy is chosen is: {:.1%}'.format(winning_strategy_prop))\n",
    "    \n",
    "    else:\n",
    "        raise ValueError('This method does not exist. Choose from eps_uniform, eps_weighted, softmax_temp')\n",
    "        \n",
    "    if (len(df.ref.unique()) == df_rec.shape[0]):\n",
    "        pass\n",
    "    else:\n",
    "        print('Recommendations = {}, Expected recommendations = {}'.format(df_rec.shape[0],len(df.ref.unique()) ))\n",
    "        raise ValueError('Incorrect number of recommendations')\n",
    "        \n",
    "        \n",
    "    df_rec = df_rec.copy().loc[:,['ref','a','y_pred','p(a)']]\n",
    "    \n",
    "    if output:\n",
    "        df_rec.to_csv('{}.csv'.format(file))\n",
    "        \n",
    "    return df_rec\n",
    "\n",
    "def action_exploder(df,\n",
    "                    distinct_actions:list,\n",
    "                    prefix_col:str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a counterfactual dataframe i.e. has all the possible action policies for each ref so as to evaluate each scenario\n",
    "    Are then able to separately calculate probabilties for each & incorporate into action selection strategy\n",
    "    \"\"\"\n",
    "    action_df = pd.DataFrame({'Action':distinct_actions})\n",
    "    dummy_combinations = pd.get_dummies(action_df['Action'],prefix=prefix_col)\n",
    "    dummy_combinations['joining_key'] = 1\n",
    "    df['joining_key'] = 1\n",
    "    all_combos = dummy_combinations.merge(df[['ref','joining_key']], on='joining_key',how='outer')\n",
    "    # JOIN DUMMIES SO INPUT SPACE ALIGNS WITH LGBM CLASSIFIER FOR PREDICTIONS #\n",
    "    df_exploded = pd.merge(df,all_combos,how='inner',on='ref').drop(columns=['joining_key_x','joining_key_y'])\n",
    "    # Check # \n",
    "    if len(distinct_actions)*df.shape[0] == df_exploded.shape[0]:\n",
    "        print('Correct number of actions created')\n",
    "    else:\n",
    "        raise ValueError('Incorrect number of actions created')\n",
    "    \n",
    "    return df_exploded\n",
    "\n",
    "def preprocess(df,\n",
    "               cat_col:str,\n",
    "               prefix_col:str) -> pd.DataFrame:\n",
    "    df = pd.concat([df.drop(columns=cat_col, inplace=False),\n",
    "                    pd.get_dummies(df[cat_col],prefix=prefix_col)],\n",
    "                   axis=1)\n",
    "    return df\n",
    "\n",
    "def model_eval(df,\n",
    "               target:pd.Series,\n",
    "               model_features:list,\n",
    "               model:any,\n",
    "               data_descr:str):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    probas = model.predict_proba(df[model_features])[:,1]\n",
    "    preds = model.predict(df[model_features])\n",
    "    print('{} AUC: {}'.format(data_descr, roc_auc_score(target,probas)))\n",
    "    print('{} Accuracy: {}'.format(data_descr, accuracy_score(target,preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historic data to learn from #\n",
    "training_data = pd.read_csv('https://hr-projects-assets-prod.s3.amazonaws.com/dh0sr099ohk/d1509ac0ad592a793b6b56456fe226aa/training_data.csv', index_col=0)\n",
    "training_data['ref'] = training_data.index\n",
    "\n",
    "# Future-looking data to apply my policy on #\n",
    "unseen_data = pd.read_csv('https://hr-projects-assets-prod.s3.amazonaws.com/dh0sr099ohk/fb501b17739e3c5457da79dd1b155b8c/unseen_data.csv', index_col=0)\n",
    "unseen_data['ref'] = unseen_data.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the data\n",
    "- Ordinarily would spend more time here where real world variables & domain knowledge but given CMAB task priority and data synthesised, will proceed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # historic_data # \n",
    "training_data.head(5)\n",
    "training_data.shape # 506K \n",
    "training_data.day.value_counts(normalize=True)\n",
    "training_data['a'].value_counts() # 1-4\n",
    "training_data['P(a)'].value_counts() # only 4 distinct probabilities \n",
    "training_data.groupby('a')['P(a)'].mean() # 2 seems to have biggest payoff on avg across all observations, followed by 3\n",
    "training_data['P(a)'].max() # 31%\n",
    "training_data['P(a)'].min() # 16%\n",
    "# Unseen & unlabelled data #\n",
    "unseen_data.head(5)\n",
    "unseen_data.shape # 65K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_actions = training_data['a'].unique().tolist()\n",
    "distinct_actions.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create dummy features using the action/policy field__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = preprocess(training_data,'a','choice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test split & Temporal Considerations\n",
    "\n",
    "- Want to assess model performance on out-of-sample & in-time set (OOS) & out-of-time set (OOT)\n",
    "\n",
    "- Options\n",
    "    1. OOS & OOT Split (CHOSEN APPROACH)\n",
    "        - Approx 70% Train, 20% Test, 10% OOT Test\n",
    "        - Can then better assess how well the model will generalise to unseen data in current period\n",
    "        - And will then be able to assess my action-selection approach vs the current existing CMAB approach\n",
    "            - would not be fair to use data after the event to train the model as a form of leakage\n",
    "            - hence why train model on subperiod and compare performance in OOT so can better benchmark against current approach\n",
    "            - benchmarking done using probability predictions since cannot of course obtain counterfactual outcomes\n",
    "    2. OOS Split Only\n",
    "        - 70% Train : 30% Test\n",
    "        - Allows me to train data on most up-to-date period, so should better predict current + future period(s)\n",
    "        - N.B Could potentially weight samples when fitting LGBM using 'day'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal Split #\n",
    "oot_validation_df = training_data.copy().loc[training_data['day'] == 8,].drop(columns='day')\n",
    "intime_df = training_data.copy().loc[training_data['day'] < 8,].drop(columns='day')\n",
    "\n",
    "# shuffle data #\n",
    "intime_df = intime_df.copy().sample(frac=1,random_state=42)\n",
    "\n",
    "# Split into X:y #\n",
    "X = intime_df.copy().drop(columns=['y','ref'])\n",
    "y = intime_df['y']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Model Fitting__\n",
    "\n",
    "- Would ordinarily compare multiple off-the-shelf algorithms using their default config to see if one algorithm was starkly more performant:\n",
    "    - logistic\n",
    "    - random forest\n",
    "    - Boosting (XGB/LGBM/....)\n",
    "    \n",
    "- Might consider including a weighting in the model fitting process according to the recency of the results \n",
    "    - i.e. the larger the 'day' feature, the higher importance placed on correcting any residual error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextual_features = [i for i in X.columns if 'x' in i]\n",
    "decision_variable = [i for i in X.columns if 'choice' in i]\n",
    "\n",
    "lgb_clf = lgbm.LGBMClassifier()\n",
    "lgb_clf.fit(X_train[contextual_features + decision_variable], y_train) # ,sample_weight = X_train['day']) # or could weight according to p(a) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval(X_train,target=y_train,model_features=lgb_clf.feature_name_,model=lgb_clf,data_descr='Train')\n",
    "model_eval(X_test,target=y_test,model_features=lgb_clf.feature_name_,model=lgb_clf,data_descr='Test InTime')\n",
    "model_eval(oot_validation_df,target=oot_validation_df['y'],model_features=lgb_clf.feature_name_,model=lgb_clf,data_descr='Test OOT')\n",
    "\n",
    "# Need probas for later #\n",
    "oot_validation_df['y_pred'] = lgb_clf.predict_proba(oot_validation_df[lgb_clf.feature_name_])[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/oracle_{date}.pkl'.format(date=today_date), 'wb') as file:\n",
    "        pickle.dump(lgb_clf, file = file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Additional Steps (with more time)__\n",
    "\n",
    "- To enhance model performance \n",
    "    - Hyperparameter Tuning\n",
    "- To better validate robustness of results use different splits\n",
    "    - Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action Selection Policy #\n",
    "\n",
    "- The ultimate action is then randomly chosen using these scaled/normalized probabilities as weights\n",
    "- We take the normalized probabilities (i.e. weights) into account by sample weighting the observations by the inverse of the probability of the action that was taken\n",
    "\n",
    "### Softmax Action Selection Rules\n",
    "- Softmax maps our actions to a set of associated probabilities as is common in reinforcement learning, which is then used to randomly select the next action\n",
    "- Issue with epsilon-greedy is that when it explores it chooses equally among all actions (i.e. as likely to pick worst action as second best - the implications of the worst action could be very bad so not a good choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode out rows to consider all possibile action scenarios #\n",
    "oot_simulation = oot_validation_df.copy().loc[:,contextual_features+['ref']]\n",
    "oot_simulation = action_exploder(oot_simulation,distinct_actions,prefix_col='choice')\n",
    "\n",
    "# Predict probas for each simulation #\n",
    "oot_simulation['y_pred'] = lgb_clf.predict_proba(oot_simulation[lgb_clf.feature_name_])[:,1]\n",
    "# Normalise these so can interpret actions as probabilities #\n",
    "oot_simulation['p(a)'] = oot_simulation.groupby('ref')['y_pred'].apply(lambda x: softmax(x))\n",
    "\n",
    "# Consider subset of action selection policies #\n",
    "oot_recs_eps = action_selection_rec(oot_simulation,method='eps_uniform',epsilon=0.1,temp=None)\n",
    "oot_recs_eps_w = action_selection_rec(oot_simulation,method='eps_weighted',epsilon=0.1,temp=None)\n",
    "oot_recs_softmax = action_selection_rec(oot_simulation,method='softmax_temp',epsilon=0.1,temp=0.02) # Ideally would tune this temp param if more time #\n",
    "\n",
    "strategies_dict = {'eps_uniform':oot_recs_eps,\n",
    "                   'eps_weighted':oot_recs_eps_w,\n",
    "                   'softmax_temp':oot_recs_softmax}\n",
    "\n",
    "# Expected success rate using these strategies #\n",
    "max_success_rate = 0\n",
    "for k,v in strategies_dict.items():\n",
    "    print('######', k, '#######')\n",
    "    new_success_rate = strategies_dict[k]['y_pred'].mean()\n",
    "    print('Expected success rate: ',new_success_rate,'\\n')\n",
    "    if new_success_rate>max_success_rate:\n",
    "        max_success_rate = new_success_rate\n",
    "        top_strategy = k\n",
    "\n",
    "actual_success_rate = oot_validation_df['y'].mean()\n",
    "print('actual_success_rate: ',actual_success_rate)\n",
    "pred_success_rate = oot_validation_df['y_pred'].mean()\n",
    "print('pred_success_rate: ',pred_success_rate)\n",
    "\n",
    "print('Action strategy is {}'.format(top_strategy))\n",
    "print('Uplift in success rate for OOT sample of {:.2%}'.format((max_success_rate-pred_success_rate)/pred_success_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply the chosen strategy on the unseen data set ##\n",
    "\n",
    "- The top strategy was to use the softmax temperature parameter setup\n",
    "- However, this was starkly different to the current action selection policy in place and may have led to too much of a radical change\n",
    "- Instead, have opted for a more conservative approach by choosing the greedy epsilon with weighted sampling approach\n",
    "- In this way can better balance exploit vs explore trade-off, and if finding performing well in real world can move onto something perhaps more radical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_data = action_exploder(unseen_data,distinct_actions,prefix_col='choice')\n",
    "# Predict probas \n",
    "unseen_data['y_pred'] = lgb_clf.predict_proba(unseen_data[lgb_clf.feature_name_])[:,1]\n",
    "# Normalise these so can interpret actions as probabilities # \n",
    "unseen_data['p(a)'] = unseen_data.groupby('ref')['y_pred'].apply(lambda x: softmax(x))\n",
    "\n",
    "##################################################\n",
    "unseen_recs_eps = action_selection_rec(unseen_data,method='eps_weighted',epsilon=0.1,temp=None,seed=42,output=True,file='QA_output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a class so this can easily be implemented in a productionised pipeline\n",
    "- To \"serve\" the model in production & provides recommended actions for new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextualBanditScoringObject():\n",
    "    \n",
    "    \"\"\"\n",
    "    Given an input feature set, recommends the best action/policy per observation (be it user/customer etc.) \n",
    "    where best is defined according to our exploitation vs exploration appetite\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 model_object: Any,\n",
    "                 df: pd.DataFrame):\n",
    "        \"\"\" \"\"\"\n",
    "        self.model_object = model_object\n",
    "        self.df = df\n",
    "\n",
    "    def preprocess(self,\n",
    "                   cat_col:str,\n",
    "                   prefix_col:str):\n",
    "        \"\"\"\n",
    "        This would ordinarily contain a bunch of preprocessing steps,\n",
    "        but since data is synthetically generated (and w/o domain knowledge) limited preprocessing is applied\n",
    "        \"\"\"\n",
    "        self.df = pd.concat([self.df.drop(columns=cat_col, inplace=False),\n",
    "                             pd.get_dummies(self.df[cat_col],prefix=prefix_col)],\n",
    "                            axis=1)\n",
    "        \n",
    "    def action_exploder(self,\n",
    "                        distinct_actions:list,\n",
    "                        prefix_col:str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Creates a counterfactual dataframe i.e. has all the possible action policies for each ref so as to evaluate each scenario\n",
    "        Are then able to separately calculate probabilties for each & incorporate into action selection strategy\n",
    "        \"\"\"\n",
    "        action_df = pd.DataFrame({'Action':distinct_actions})\n",
    "        dummy_combinations = pd.get_dummies(action_df['Action'],prefix=prefix_col)\n",
    "        dummy_combinations['joining_key'] = 1\n",
    "        self.df['joining_key'] = 1\n",
    "        all_combos = dummy_combinations.merge(self.df[['ref','joining_key']], on='joining_key',how='outer')\n",
    "        # JOIN DUMMIES SO INPUT SPACE ALIGNS WITH LGBM CLASSIFIER FOR PREDICTIONS #\n",
    "        df_exploded = pd.merge(self.df,all_combos,how='inner',on='ref').drop(columns=['joining_key_x','joining_key_y'])\n",
    "        # Check # \n",
    "        if len(distinct_actions)*self.df.shape[0] == df_exploded.shape[0]:\n",
    "            print('Correct number of actions created')\n",
    "        else:\n",
    "            raise ValueError('Incorrect number of actions created')\n",
    "            \n",
    "        self.df = df_exploded.copy()\n",
    "    \n",
    "    def predict(self):\n",
    "        \"\"\" \n",
    "        Predict probabilities of contextual information and associated action given a fitted model\n",
    "        Also predicts latent probabilities of actions\n",
    "        \"\"\"\n",
    "        self.df['y_pred'] = self.model_object.predict_proba(self.df[self.model_object.feature_name_])[:,1] # may not work if different object type\n",
    "        # Normalise these so can interpret actions as probabilities #\n",
    "        self.df['p(a)'] = self.df.groupby('ref')['y_pred'].apply(lambda x: softmax(x))\n",
    "    \n",
    "    def action_selection_rec(self,\n",
    "                         method:str,\n",
    "                         epsilon=0.1,\n",
    "                         temp=0.02,\n",
    "                         seed=42,\n",
    "                         output=False,\n",
    "                         file=None) -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "        df: dataframe containing refs, actions and probability of success for each respective action\n",
    "        method: eps_uniform, eps_weighted, softmax_temp\n",
    "        eps_uniform = Epsilon-greedy with uniformly sampled exploratory action\n",
    "        eps_weighted = Epsilon-greedy with weight-based sampling of exploratory action\n",
    "        softmax_temp = After applying the softmax transformation (including the addition of a \n",
    "                        temperature parameter that trades off between exploit vs explore), will sample actions using weights\n",
    "        return: dataframe with the recommended action for each ref, associated probability of success, and latent action probability\n",
    "        \"\"\"\n",
    "    \n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        rec_df = self.df\n",
    "        if method in ['eps_uniform','eps_weighted']:\n",
    "        \n",
    "            winning_indexes = rec_df.groupby('ref')['y_pred'].idxmax().values\n",
    "            rec_df.loc[winning_indexes,'max_action'] = 1\n",
    "            # To get scaled probas after excluding the winning strategy #\n",
    "            rec_df.loc[rec_df['max_action'] != 1,'p(a)_exploratory'] = rec_df.loc[rec_df['max_action'] != 1,].groupby('ref')['y_pred'].apply(lambda x: softmax(x))\n",
    "            exploit_refs = np.random.randint(rec_df['ref'].min(), rec_df['ref'].max(), int(len(rec_df['ref'].unique())*(1-epsilon)) )\n",
    "            explore_refs = [i for i in rec_df['ref'].unique() if i not in exploit_refs]\n",
    "            # For 1-epsilon of refs, choose action according to greedy approach\n",
    "            exploit_df = rec_df.loc[(rec_df['ref'].isin(exploit_refs)) & (rec_df['max_action']==1),]\n",
    "\n",
    "            if method == 'eps_uniform':\n",
    "                explore_df = rec_df.loc[(rec_df['ref'].isin(explore_refs)) & (rec_df['max_action']!=1), ].groupby('ref').apply(lambda x: x.sample(n=1))\n",
    "            \n",
    "            else:\n",
    "                # eps_weighted # \n",
    "                exploratory_idx = rec_df.loc[(rec_df['ref'].isin(explore_refs)) & (rec_df['max_action'] != 1), ].groupby('ref').apply(lambda x: np.random.choice(x.index,1,p=x['p(a)_exploratory'].values))\n",
    "                explore_df = rec_df.copy().loc[rec_df.index.isin(exploratory_idx),]\n",
    "            \n",
    "            df_rec = pd.concat([explore_df,exploit_df],axis=0)\n",
    "            df_rec['a'] = df_rec[[i for i in df_rec.columns if 'choice' in i]].idxmax(axis=1)\n",
    "    \n",
    "        elif method == 'softmax_temp':\n",
    "    \n",
    "            rec_df['p(a)'] = rec_df.groupby('ref')['y_pred'].apply(lambda x: softmax_temp(x.values,temp)).explode().values\n",
    "            choice_indexes = rec_df.groupby('ref').apply(lambda x: np.random.choice(x.index,1,p=x['p(a)']))\n",
    "            df_rec = rec_df.copy().loc[rec_df.index.isin(choice_indexes),]\n",
    "            df_rec['a'] = df_rec[[i for i in df_rec.columns if 'choice' in i]].idxmax(axis=1)\n",
    "            #What proportion of the times is the winning strategy chosen ? #\n",
    "            winning_strategy_prop = (df_rec['max_action'] == 1).sum() / len(df_rec)\n",
    "            print('Proportion of times winning strategy is chosen is: {:.1%}'.format(winning_strategy_prop))\n",
    "    \n",
    "        else:\n",
    "            raise ValueError('This method does not exist. Choose from eps_uniform, eps_weighted, softmax_temp')\n",
    "        \n",
    "        if (len(rec_df.ref.unique()) == df_rec.shape[0]):\n",
    "            pass\n",
    "        else:\n",
    "            print('Recommendations = {}, Expected recommendations = {}'.format(df_rec.shape[0],len(rec_df.ref.unique()) ))\n",
    "            raise ValueError('Incorrect number of recommendations')\n",
    "        \n",
    "        # Make sure it has y_pred, p(a) and a as columns #\n",
    "        df_rec = df_rec.copy().loc[:,['ref','a','y_pred','p(a)']]\n",
    "    \n",
    "        if output:\n",
    "            df_rec.to_csv('{}.csv'.format(file))\n",
    "        \n",
    "        return df_rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mockup of implementation in a production-style setting (.py file to run in the console)\n",
    "\n",
    "- Need to flesh out more to run properly, but underlines the general idea!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmab_config = {\n",
    "    'input_path':'https://hr-projects-assets-prod.s3.amazonaws.com/dh0sr099ohk/fb501b17739e3c5457da79dd1b155b8c/unseen_data.csv',\n",
    "    'model_path':'models/oracle_20230219.pkl',\n",
    "    'preprocessing_dict':{'cat_col':'a','prefix_col':'choice'},\n",
    "    'distinct_actions':[1.0,2.0,3.0,4.0],\n",
    "    'action_dict':{'method':'eps_weighted','epsilon':0.1,'temp':None,'seed':42,'output':True,'file':'unseen_data'}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__This just verifies that this runs correctly in notebook before implementing in python script__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cmab_orchestration(input_data,model_path,preprocessing_dict,distinct_actions,action_dict):\n",
    "    \n",
    "#     # Load Model #\n",
    "#     with open(\"models/oracle_20230219.pkl\", \"rb\") as input_file:\n",
    "#         oracle_model = cPickle.load(input_file)\n",
    "#     # model_file = open(model_path,'rb').close()\n",
    "#     # oracle_model = pickle.load(file)\n",
    "    \n",
    "#     # Load Data #\n",
    "#     unseen_data = pd.read_csv(input_data, index_col=0)\n",
    "#     unseen_data['ref'] = unseen_data.index\n",
    "    \n",
    "#     # Instantiate Contextual Multi-armed Bandit Class\n",
    "#     cmab = ContextualBanditScoringObject(oracle_model,unseen_data)\n",
    "#     # cmab.preprocess(**preprocessing_dict)\n",
    "#     cmab.action_exploder(distinct_actions=distinct_actions,prefix_col='choice')\n",
    "#     cmab.predict()\n",
    "#     cmab_recommendations = cmab.action_selection_rec(**action_dict)\n",
    "\n",
    "# cmab_orchestration(cmab_config['input_path'],\n",
    "#                    cmab_config['model_path'],\n",
    "#                    cmab_config['preprocessing_dict'],\n",
    "#                    cmab_config['distinct_actions'],\n",
    "#                    cmab_config['action_dict'])\n",
    "\n",
    "### QA TO CHECK THAT THIS METHOD PRODUCES SAME OUTPUTS AS NOTEBOOK APPROACH\n",
    "# merged_foo = pd.merge(foo,foo2,how='inner',on='ref')\n",
    "# np.where(merged_foo['a_x']!=merged_foo['a_y'],1,0).sum()\n",
    "# np.where(merged_foo['y_pred_x']!=merged_foo['y_pred_y'],1,0).sum()\n",
    "# np.where(merged_foo['p(a)_x']!=merged_foo['p(a)_y'],1,0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Define params in config file called cmab_config.json - see cmab_config dict above__``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argsparse\n",
    "import pickle\n",
    "import _pickle as cPickle\n",
    "# etc. # \n",
    "\n",
    "def cmab_orchestration(input_data,model_path,preprocessing_dict,distinct_actions,action_dict):\n",
    "    \n",
    "    # Load Model #\n",
    "    with open(model_path, 'rb') as input_file:\n",
    "        oracle_model = cPickle.load(input_file)\n",
    "    \n",
    "    # Load Data #\n",
    "    unseen_data = pd.read_csv(input_data, index_col=0)\n",
    "    unseen_data['ref'] = unseen_data.index\n",
    "    \n",
    "    # Instantiate Contextual Multi-armed Bandit Class\n",
    "    cmab = ContextualBanditScoringObject(oracle_model,unseen_data)\n",
    "    # cmab.preprocess(**preprocessing_dict) # this step is only required for the historic train set #\n",
    "    cmab.action_exploder(distinct_actions=distinct_actions,prefix_col='choice')\n",
    "    cmab.predict()\n",
    "    cmab_recommendations = cmab.action_selection_rec(**action_dict)\n",
    "\n",
    "def main():\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='CMAB Recommendations')\n",
    "    parser.add_argument('--setup_file', type=str, help='Path to the json configuration file, to determine inputs and preference for exploitation vs exploration')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    with open(args.setup_file) as json_file:\n",
    "        config_dictionary = json.load(json_file) # this would be the cmab_config dictionary as per above\n",
    "    \n",
    "    cmab_orchestration(config_dictionary['input_path'],\n",
    "                       config_dictionary['model_path'],\n",
    "                       config_dictionary['preprocessing_dict'],\n",
    "                       config_dictionary['distinct_actions'],\n",
    "                       config_dictionary['action_dict']\n",
    "                      )\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    main()\n",
    "    \n",
    "# $ python3 cmab_script.py --setup_file cmab_config.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7 (tags/v3.8.7:6503f05, Dec 21 2020, 17:59:51) [MSC v.1928 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fa7f07b01af53b436023be1dbcc9dd9b374e3f104e328a5cf3bf59b1af800e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
